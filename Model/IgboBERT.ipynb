{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IboBERT",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chiamakac/IgboNER-Models/blob/main/Model/IgboBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training an Igbo language model from scratch using Transformers and Tokenizers**"
      ],
      "metadata": {
        "id": "4eudk9KP37g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Getting the data.**"
      ],
      "metadata": {
        "id": "fuK92URk4X7f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4amslz4Oo9vT"
      },
      "source": [
        "!wget -c https://github.com/IgnatiusEzeani/IGBONLP/raw/master/ig_monoling/text.zip\n",
        "!wget -c https://raw.githubusercontent.com/chiamaka249/lacuna_pos_ner/main/language_corpus/ibo/ibo.txt\n",
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IboBETA/main/config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsSkvlYoplYP"
      },
      "source": [
        "#Unzip the zipped file and remove the zipped file after unzipping\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "def unzip(zipfilename):\n",
        "  try:\n",
        "    with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "      zip_ref.extractall(zipfilename[:-4])\n",
        "      return f\"'{zipfilename}' unzipped!\"\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Cannot find '{zipfilename}' file\")\n",
        "\n",
        "unzip(\"text.zip\")\n",
        "!rm text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua0R2p31p8zA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2676fd77-b437-4442-8af0-be4d301d681a"
      },
      "source": [
        "#copies the file \"ibo.txt\" into the folder \"text\"\n",
        "import shutil\n",
        "shutil.move('/content/ibo.txt', '/content/text')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/text/ibo.txt'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj4i2-9bp4U3"
      },
      "source": [
        "\n",
        "# import os\n",
        "#import shutil\n",
        "dir_name = \"/content/text\"\n",
        "text=\"\"\n",
        "for fname in os.listdir(dir_name):\n",
        "  fname = os.path.join(dir_name, fname)\n",
        "  with open(fname, \"r\", encoding=\"utf8\") as datafile:\n",
        "    text = text+\"\\n\"+datafile.read()\n",
        "\n",
        "with open(\"data.txt\", \"w\", encoding=\"utf8\") as datafile:\n",
        "  datafile.write(text)\n",
        "\n",
        "shutil.rmtree(\"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Import Transformers, Tokenizer and Train the tokenizer**"
      ],
      "metadata": {
        "id": "kEMNI_qD9lq3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLhNvJubEowT"
      },
      "source": [
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "\n",
        "# Install `transformers` from master stating the version\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "\n",
        "# transformers version at notebook update --- 2.11.0\n",
        "# tokenizers version at notebook update --- 0.8.0rc1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyraD86RE3QK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb67e7f7-7f52-4081-ec7c-e15f5baf742b"
      },
      "source": [
        "\n",
        "%%time \n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Describing the path to all of our Igbo data \n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "print(paths)\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",#Beginning of sequence (BOS) or classifier (CLS) token\n",
        "    \"<pad>\",# Padding token\n",
        "    \"</s>\",#End of sequence (EOS) or seperator (SEP) token\n",
        "    \"<unk>\",# Unknown token\n",
        "    \"<mask>\", # Masking token\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data.txt']\n",
            "CPU times: user 18.9 s, sys: 1.2 s, total: 20.1 s\n",
            "Wall time: 6.07 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro52g8BqFFfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dbe9f4-3ec6-4226-a44f-85249e7b0afb"
      },
      "source": [
        "#Our tokenizer is now ready and we have two files that define our new IgboBert tokenizer( a vocab.json-which is a list of the most frequent tokens ranked by frequency and a merges.txt list of merges)\n",
        "#we then save the file for later use\n",
        "\n",
        "!mkdir IgboBert\n",
        "tokenizer.save_model(\"IgboBert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['IgboBert/vocab.json', 'IgboBert/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.move('/content/config.json', '/content/IgboBert')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PtLNA80d4Olp",
        "outputId": "e60d6c56-3f90-4f59-c45e-88ed1e24b357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/IgboBert/config.json'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Initializing the Tokenizer**\n",
        "\n",
        "Let's initialize our tokenizer. This way we can use it as we would use any other from_pretrained tokenizer."
      ],
      "metadata": {
        "id": "KSuZYOW7NQP8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FekvedLrFR_t"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./IgboBert/vocab.json\",\n",
        "    \"./IgboBert/merges.txt\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E52LgvLWFbuq"
      },
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nm7h9ndFis2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fbc3757-ea78-43b6-ca12-fb30cf08fff8"
      },
      "source": [
        "tokenizer.encode(\"Simone gara ụka ụnyahụ guọ egwu ma ga-kwa taa.\", \"Aha ya bụ ifeoma.\").tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'Simone',\n",
              " 'Ġgara',\n",
              " 'Ġá»¥ka',\n",
              " 'Ġá»¥nyahá»¥',\n",
              " 'Ġgu',\n",
              " 'á»į',\n",
              " 'Ġegwu',\n",
              " 'Ġma',\n",
              " 'Ġga',\n",
              " '-',\n",
              " 'kwa',\n",
              " 'Ġtaa',\n",
              " '.',\n",
              " '</s>',\n",
              " 'Aha',\n",
              " 'Ġya',\n",
              " 'Ġbá»¥',\n",
              " 'Ġife',\n",
              " 'oma',\n",
              " '.',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxcYhcxRKROn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe65a457-95c2-4313-8a84-779fe28916c3"
      },
      "source": [
        "# Check that we have a GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 15 20:56:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMe5vRLvG5Zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf36130-8252-4815-d7b5-009eb5b738e2"
      },
      "source": [
        "# Check that PyTorch sees it\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUtbKfFgG-Y3"
      },
      "source": [
        "#For training, we need a raw (not pre-trained) BERTLMHeadModel. \n",
        "#To create that, we first need to create a RoBERTa config object to describe the parameters we’d like to initialize IgboBERT with.\n",
        "\n",
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIqujlqjHQnX"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"./IgboBert\", max_len=512, config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvup7wl8Hhp9"
      },
      "source": [
        "#We import and initialize our RoBERTa model with a language modeling (LM) head.\n",
        "\n",
        "from transformers import RobertaForMaskedLM\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyWUosTlHnRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21dee24f-eda3-4536-db2f-a45ea15ee120"
      },
      "source": [
        "model.num_parameters()\n",
        "# => 83 million parameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83504416"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujpbn1oHp-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48e387e-4589-449b-e1d9-68c4a68243e9"
      },
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    file_path = \"/content/data.txt\",\n",
        "    block_size = 128\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:125: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 33.3 s, sys: 1.27 s, total: 34.5 s\n",
            "Wall time: 15.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EICtSzqwH618"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kalucrRPH9wb"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./IgboBert\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCWG7CRZIOkb"
      },
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkwVjpAyIVu9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df90570-f57d-489f-9329-68da775e2c90"
      },
      "source": [
        "trainer.save_model(\"./IgboBert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./IgboBert\n",
            "Configuration saved in ./IgboBert/config.json\n",
            "Model weights saved in ./IgboBert/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Test the Model**\n",
        "\n",
        "We first initialize a pipeline object, using the 'fill-mask' argument. Then begin testing our model like so\n",
        "\n"
      ],
      "metadata": {
        "id": "RS_L5EpBYJvF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYx9FK7ZIYZs"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./IgboBert\",\n",
        "    tokenizer=\"./IgboBert\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQK6jyf9IkRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3a764f-2dae-4513-909e-1cf5edc59a1d"
      },
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Abụ m Maazị <mask>.\") #= okafor/Ọkafọ\n",
        "# fill_mask(\"Nwaanyị na <mask> ji na akara.\") #=eri"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.008856686763465405,\n",
              "  'sequence': 'Abụ m Maazị Mohammed.',\n",
              "  'token': 3231,\n",
              "  'token_str': ' Mohammed'},\n",
              " {'score': 0.007911812514066696,\n",
              "  'sequence': 'Abụ m Maazị O.',\n",
              "  'token': 381,\n",
              "  'token_str': ' O'},\n",
              " {'score': 0.00748562254011631,\n",
              "  'sequence': 'Abụ m Maazị Ọkafọ.',\n",
              "  'token': 5307,\n",
              "  'token_str': ' Ọkafọ'},\n",
              " {'score': 0.006802904885262251,\n",
              "  'sequence': 'Abụ m Maazị A.',\n",
              "  'token': 348,\n",
              "  'token_str': ' A'},\n",
              " {'score': 0.0032701685559004545,\n",
              "  'sequence': 'Abụ m Maazị Rutherford.',\n",
              "  'token': 5113,\n",
              "  'token_str': ' Rutherford'}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0mje0nMIoWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd4723c-a3eb-4cc3-8c35-1bfb14cb421c"
      },
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Nwaanyị na <mask> ji na akara.\") #= eri\n",
        "# fill_mask(\"Nwaanyị na <mask> ji na akara.\") #=eri"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1661785989999771,\n",
              "  'sequence': 'Nwaanyị na ya ji na akara.',\n",
              "  'token': 289,\n",
              "  'token_str': ' ya'},\n",
              " {'score': 0.11589646339416504,\n",
              "  'sequence': 'Nwaanyị na nwaanyị ji na akara.',\n",
              "  'token': 623,\n",
              "  'token_str': ' nwaanyị'},\n",
              " {'score': 0.03902192786335945,\n",
              "  'sequence': 'Nwaanyị na nwunye ji na akara.',\n",
              "  'token': 724,\n",
              "  'token_str': ' nwunye'},\n",
              " {'score': 0.038711633533239365,\n",
              "  'sequence': 'Nwaanyị na nna ji na akara.',\n",
              "  'token': 713,\n",
              "  'token_str': ' nna'},\n",
              " {'score': 0.02920209988951683,\n",
              "  'sequence': 'Nwaanyị na- ji na akara.',\n",
              "  'token': 17,\n",
              "  'token_str': '-'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr9wjE8PItpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1aed77-ff16-4e7f-c001-888d53f984b5"
      },
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Chineke ga- ebibikwa ndị niile na- eme ihe <mask>.\") #=ọjọọ\n",
        "# fill_mask(\"Nwaanyị na <mask> ji na akara.\") #=eri"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.24352985620498657,\n",
              "  'sequence': 'Chineke ga- ebibikwa ndị niile na- eme ihe ọma.',\n",
              "  'token': 496,\n",
              "  'token_str': ' ọma'},\n",
              " {'score': 0.16800811886787415,\n",
              "  'sequence': 'Chineke ga- ebibikwa ndị niile na- eme ihe ọjọọ.',\n",
              "  'token': 707,\n",
              "  'token_str': ' ọjọọ'},\n",
              " {'score': 0.15601330995559692,\n",
              "  'sequence': 'Chineke ga- ebibikwa ndị niile na- eme ihe a.',\n",
              "  'token': 266,\n",
              "  'token_str': ' a'},\n",
              " {'score': 0.11959126591682434,\n",
              "  'sequence': 'Chineke ga- ebibikwa ndị niile na- eme ihe niile.',\n",
              "  'token': 427,\n",
              "  'token_str': ' niile'},\n",
              " {'score': 0.02550322934985161,\n",
              "  'sequence': 'Chineke ga- ebibikwa ndị niile na- eme ihe oriri.',\n",
              "  'token': 1580,\n",
              "  'token_str': ' oriri'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubR7pCxJIyNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a99ff7-904b-4551-dbd1-925c53b39399"
      },
      "source": [
        "fill_mask(\"ọba akwụkwọ Ọkammụta Kenneth Dike dị <mask>.\") #n'Awka\n",
        "\n",
        "# This is the beginning of a beautiful <mask>.\n",
        "# =>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.10755623877048492,\n",
              "  'sequence': 'ọba akwụkwọ Ọkammụta Kenneth Dike dị mkpa.',\n",
              "  'token': 607,\n",
              "  'token_str': ' mkpa'},\n",
              " {'score': 0.07236427068710327,\n",
              "  'sequence': 'ọba akwụkwọ Ọkammụta Kenneth Dike dị nso.',\n",
              "  'token': 604,\n",
              "  'token_str': ' nso'},\n",
              " {'score': 0.067164845764637,\n",
              "  'sequence': 'ọba akwụkwọ Ọkammụta Kenneth Dike dị mma.',\n",
              "  'token': 347,\n",
              "  'token_str': ' mma'},\n",
              " {'score': 0.054604168981313705,\n",
              "  'sequence': 'ọba akwụkwọ Ọkammụta Kenneth Dike dị iche.',\n",
              "  'token': 462,\n",
              "  'token_str': ' iche'},\n",
              " {'score': 0.04094060882925987,\n",
              "  'sequence': 'ọba akwụkwọ Ọkammụta Kenneth Dike dị ukwuu.',\n",
              "  'token': 1009,\n",
              "  'token_str': ' ukwuu'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgveMBQYNZV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c0ff50-c1ef-4a27-88ee-1148af67f588"
      },
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Nwaanyị na eri <mask> na akara.\") #= ji\n",
        "# fill_mask(\"Nwaanyị na <mask> ji na akara.\") #=eri"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.12033308297395706,\n",
              "  'sequence': 'Nwaanyị na eri ya na akara.',\n",
              "  'token': 289,\n",
              "  'token_str': ' ya'},\n",
              " {'score': 0.07522733509540558,\n",
              "  'sequence': 'Nwaanyị na eri nri na akara.',\n",
              "  'token': 870,\n",
              "  'token_str': ' nri'},\n",
              " {'score': 0.0471433624625206,\n",
              "  'sequence': 'Nwaanyị na eri ihe na akara.',\n",
              "  'token': 300,\n",
              "  'token_str': ' ihe'},\n",
              " {'score': 0.02259232848882675,\n",
              "  'sequence': 'Nwaanyị na eri eri na akara.',\n",
              "  'token': 957,\n",
              "  'token_str': ' eri'},\n",
              " {'score': 0.01788548193871975,\n",
              "  'sequence': 'Nwaanyị na eri nwaanyị na akara.',\n",
              "  'token': 623,\n",
              "  'token_str': ' nwaanyị'}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Gaanụ mee ndị <mask> niile ka ha bụrụ ndị na- eso ụzọ m  .\") #= mba\n"
      ],
      "metadata": {
        "id": "-JgRzOtZmTtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38fc22c-dcb1-43f8-b98c-6de4a997886d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3522748351097107,\n",
              "  'sequence': 'Gaanụ mee ndị a niile ka ha bụrụ ndị na- eso ụzọ m .',\n",
              "  'token': 266,\n",
              "  'token_str': ' a'},\n",
              " {'score': 0.08272776752710342,\n",
              "  'sequence': 'Gaanụ mee ndị ọzọ niile ka ha bụrụ ndị na- eso ụzọ m .',\n",
              "  'token': 434,\n",
              "  'token_str': ' ọzọ'},\n",
              " {'score': 0.06121758744120598,\n",
              "  'sequence': 'Gaanụ mee ndị ahụ niile ka ha bụrụ ndị na- eso ụzọ m .',\n",
              "  'token': 310,\n",
              "  'token_str': ' ahụ'},\n",
              " {'score': 0.06120830774307251,\n",
              "  'sequence': 'Gaanụ mee ndị mmadụ niile ka ha bụrụ ndị na- eso ụzọ m .',\n",
              "  'token': 393,\n",
              "  'token_str': ' mmadụ'},\n",
              " {'score': 0.04012814536690712,\n",
              "  'sequence': 'Gaanụ mee ndị Izrel niile ka ha bụrụ ndị na- eso ụzọ m .',\n",
              "  'token': 680,\n",
              "  'token_str': ' Izrel'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Jehova họpụtara Mozis ka ọ bụrụ onye ndú ụmụ <mask>.\") #= Izrel\n"
      ],
      "metadata": {
        "id": "_zB0aW-cmuQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103711b5-3d4d-4408-b27b-f543735aeb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3820941150188446,\n",
              "  'sequence': 'Jehova họpụtara Mozis ka ọ bụrụ onye ndú ụmụ Izrel.',\n",
              "  'token': 680,\n",
              "  'token_str': ' Izrel'},\n",
              " {'score': 0.2837800979614258,\n",
              "  'sequence': 'Jehova họpụtara Mozis ka ọ bụrụ onye ndú ụmụ ya.',\n",
              "  'token': 289,\n",
              "  'token_str': ' ya'},\n",
              " {'score': 0.10724257677793503,\n",
              "  'sequence': 'Jehova họpụtara Mozis ka ọ bụrụ onye ndú ụmụ mmadụ.',\n",
              "  'token': 393,\n",
              "  'token_str': ' mmadụ'},\n",
              " {'score': 0.024513551965355873,\n",
              "  'sequence': 'Jehova họpụtara Mozis ka ọ bụrụ onye ndú ụmụ ha.',\n",
              "  'token': 296,\n",
              "  'token_str': ' ha'},\n",
              " {'score': 0.01360291987657547,\n",
              "  'sequence': 'Jehova họpụtara Mozis ka ọ bụrụ onye ndú ụmụ Igbo.',\n",
              "  'token': 900,\n",
              "  'token_str': ' Igbo'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Ụmụakwụkwọ Chibok anọọla ụbọchị 2000 n’ aka <mask> Haram.\") #= Boko\n"
      ],
      "metadata": {
        "id": "BD8wh6YRmtbs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77835576-ece5-43c9-aa0d-100fe95077f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.8887956738471985,\n",
              "  'sequence': 'Ụmụakwụkwọ Chibok anọọla ụbọchị 2000 n’ aka Boko Haram.',\n",
              "  'token': 2535,\n",
              "  'token_str': ' Boko'},\n",
              " {'score': 0.007407982833683491,\n",
              "  'sequence': 'Ụmụakwụkwọ Chibok anọọla ụbọchị 2000 n’ aka Manchester Haram.',\n",
              "  'token': 3278,\n",
              "  'token_str': ' Manchester'},\n",
              " {'score': 0.007324530277401209,\n",
              "  'sequence': 'Ụmụakwụkwọ Chibok anọọla ụbọchị 2000 n’ aka Super Haram.',\n",
              "  'token': 3199,\n",
              "  'token_str': ' Super'},\n",
              " {'score': 0.0033930952195078135,\n",
              "  'sequence': 'Ụmụakwụkwọ Chibok anọọla ụbọchị 2000 n’ aka ịgba Haram.',\n",
              "  'token': 874,\n",
              "  'token_str': ' ịgba'},\n",
              " {'score': 0.0025665571447461843,\n",
              "  'sequence': 'Ụmụakwụkwọ Chibok anọọla ụbọchị 2000 n’ aka Gọvanọ Haram.',\n",
              "  'token': 1692,\n",
              "  'token_str': ' Gọvanọ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Nwunye Gọvanọ Ekiti steeti bụ Bisi Fayemi so na ndị na- akwado <mask> ọhụrụ a.\") #= iwu\n"
      ],
      "metadata": {
        "id": "k6sdE6Wemswq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8d7c9b-2f39-4e3d-d5a2-7c919001a35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.1165534108877182,\n",
              "  'sequence': 'Nwunye Gọvanọ Ekiti steeti bụ Bisi Fayemi so na ndị na- akwado ọchịchị ọhụrụ a.',\n",
              "  'token': 719,\n",
              "  'token_str': ' ọchịchị'},\n",
              " {'score': 0.056705061346292496,\n",
              "  'sequence': 'Nwunye Gọvanọ Ekiti steeti bụ Bisi Fayemi so na ndị na- akwado ọrụ ọhụrụ a.',\n",
              "  'token': 477,\n",
              "  'token_str': ' ọrụ'},\n",
              " {'score': 0.04727887734770775,\n",
              "  'sequence': 'Nwunye Gọvanọ Ekiti steeti bụ Bisi Fayemi so na ndị na- akwado ọnọdụ ọhụrụ a.',\n",
              "  'token': 1036,\n",
              "  'token_str': ' ọnọdụ'},\n",
              " {'score': 0.042590655386447906,\n",
              "  'sequence': 'Nwunye Gọvanọ Ekiti steeti bụ Bisi Fayemi so na ndị na- akwado ụwa ọhụrụ a.',\n",
              "  'token': 556,\n",
              "  'token_str': ' ụwa'},\n",
              " {'score': 0.0251015517860651,\n",
              "  'sequence': 'Nwunye Gọvanọ Ekiti steeti bụ Bisi Fayemi so na ndị na- akwado obodo ọhụrụ a.',\n",
              "  'token': 576,\n",
              "  'token_str': ' obodo'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\" <mask> sị ka ehiwe ụlọikpe pụrụiche maka mpụ.\") #= Buhari\n"
      ],
      "metadata": {
        "id": "X8e7jRBLmrtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3219736a-eee0-46a7-fc32-ea3cf88fad13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.44573667645454407,\n",
              "  'sequence': 'A sị ka ehiwe ụlọikpe pụrụiche maka mpụ.',\n",
              "  'token': 37,\n",
              "  'token_str': 'A'},\n",
              " {'score': 0.230531707406044,\n",
              "  'sequence': 'Ọ sị ka ehiwe ụlọikpe pụrụiche maka mpụ.',\n",
              "  'token': 336,\n",
              "  'token_str': 'Ọ'},\n",
              " {'score': 0.03158653527498245,\n",
              "  'sequence': 'Ha sị ka ehiwe ụlọikpe pụrụiche maka mpụ.',\n",
              "  'token': 513,\n",
              "  'token_str': 'Ha'},\n",
              " {'score': 0.02112211100757122,\n",
              "  'sequence': 'Igbo sị ka ehiwe ụlọikpe pụrụiche maka mpụ.',\n",
              "  'token': 3656,\n",
              "  'token_str': 'Igbo'},\n",
              " {'score': 0.011359370313584805,\n",
              "  'sequence': 'O sị ka ehiwe ụlọikpe pụrụiche maka mpụ.',\n",
              "  'token': 51,\n",
              "  'token_str': 'O'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Ala <mask>  ga- eweta ezi ọnọdụ nchekwa maka ndị chọrọ ịwebata ego n’ ọrụ ugbo.\") #= Naịjirịa\n"
      ],
      "metadata": {
        "id": "b1uSDAbWmq3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4104903e-6503-4b8f-fe99-a5c0dfc6852b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.10420944541692734,\n",
              "  'sequence': 'Ala a  ga- eweta ezi ọnọdụ nchekwa maka ndị chọrọ ịwebata ego n’ ọrụ ugbo.',\n",
              "  'token': 266,\n",
              "  'token_str': ' a'},\n",
              " {'score': 0.07349573075771332,\n",
              "  'sequence': 'Ala ahụ  ga- eweta ezi ọnọdụ nchekwa maka ndị chọrọ ịwebata ego n’ ọrụ ugbo.',\n",
              "  'token': 310,\n",
              "  'token_str': ' ahụ'},\n",
              " {'score': 0.030463453382253647,\n",
              "  'sequence': 'Ala Igbo  ga- eweta ezi ọnọdụ nchekwa maka ndị chọrọ ịwebata ego n’ ọrụ ugbo.',\n",
              "  'token': 900,\n",
              "  'token_str': ' Igbo'},\n",
              " {'score': 0.02490180917084217,\n",
              "  'sequence': 'Ala Nigeria  ga- eweta ezi ọnọdụ nchekwa maka ndị chọrọ ịwebata ego n’ ọrụ ugbo.',\n",
              "  'token': 1570,\n",
              "  'token_str': ' Nigeria'},\n",
              " {'score': 0.01187092810869217,\n",
              "  'sequence': 'Ala ọrụ  ga- eweta ezi ọnọdụ nchekwa maka ndị chọrọ ịwebata ego n’ ọrụ ugbo.',\n",
              "  'token': 477,\n",
              "  'token_str': ' ọrụ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRbvsFIyNpID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31991bc9-6577-483d-d9bc-f0d68373fb44"
      },
      "source": [
        "# The sun <mask>.\n",
        "# =>\n",
        "\n",
        "fill_mask(\"Ọ bụ <mask>a ka a na- arịa .\") #= mmadụ\n",
        "# fill_mask(\"Nwaanyị na <mask> ji na akara.\") #=eri"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.044008973985910416,\n",
              "  'sequence': 'Ọ bụ-a ka a na- arịa.',\n",
              "  'token': 17,\n",
              "  'token_str': '-'},\n",
              " {'score': 0.0439983569085598,\n",
              "  'sequence': 'Ọ bụ nwaa ka a na- arịa.',\n",
              "  'token': 419,\n",
              "  'token_str': ' nwa'},\n",
              " {'score': 0.041657522320747375,\n",
              "  'sequence': 'Ọ bụ Nwannaa ka a na- arịa.',\n",
              "  'token': 1459,\n",
              "  'token_str': ' Nwanna'},\n",
              " {'score': 0.018149062991142273,\n",
              "  'sequence': 'Ọ bụ ọrịaa ka a na- arịa.',\n",
              "  'token': 956,\n",
              "  'token_str': ' ọrịa'},\n",
              " {'score': 0.009843925014138222,\n",
              "  'sequence': 'Ọ bụ naanịa ka a na- arịa.',\n",
              "  'token': 769,\n",
              "  'token_str': ' naanị'}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Hddc-BufgXzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903e12a9-703c-42e5-d7b0-45e98b4f9844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#move model to gdrive\n",
        "shutil.move('model path','drive path')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GvyCrVFLwlLZ",
        "outputId": "1ce40fa6-2616-44b5-e0c8-bdf5cd5ecb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/IBO_BETA/IgboBert'"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}