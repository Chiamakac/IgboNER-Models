{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chiamakac/IgboNER-Models/blob/main/Fine-Tuning/Fine_tuned_Token_Classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FqwTklG6xhs"
      },
      "source": [
        "# Token Classification (PyTorch)-Fine-Tunning "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ],
      "metadata": {
        "id": "VGFWhptXJvqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnd-ZGjN6xlB"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "#!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the followin line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9DK6qWv6xlO"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vELy8Zv6xlR"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUHL8Xb36xlY"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqSiROA56xle"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dwa6lCN6xlr"
      },
      "outputs": [],
      "source": [
        "#We use the load_dataset() method from the Datasets library to download our dataset.\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset('masakhaner', 'ibo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVf57nSO6xmK",
        "outputId": "4913a55c-9388-4116-e96c-35fc01f30dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 2235\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 320\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'ner_tags'],\n",
              "        num_rows: 638\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#shows us the columns present and the split between the training, validation, and test sets\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqjyR_YJ6xmQ",
        "outputId": "4292973a-ef5c-42b0-e529-17a9919d1f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ike',\n",
              " 'ịda',\n",
              " 'jụụ',\n",
              " 'otụ',\n",
              " 'nkeji',\n",
              " 'banyere',\n",
              " 'oke',\n",
              " 'ogbugbu',\n",
              " 'na',\n",
              " '-',\n",
              " 'eme',\n",
              " \"n'ala\",\n",
              " 'Naijiria',\n",
              " 'agwụla',\n",
              " 'Ekweremmadụ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# displays the first element of the training set\n",
        "raw_datasets[\"train\"][0][\"tokens\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpuKq5Bx6xme",
        "outputId": "c0f984a5-6d05-47b2-86ed-ccfbcd9a2959",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE'], names_file=None, id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#displays the features attribute of our dataset\n",
        "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
        "ner_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igweOAb-6xmh",
        "outputId": "cea0e184-f917-4793-975a-1e311d4a98ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#we can access the list of names in the ner_feature by looking at the names attribute of that feature\n",
        "label_names = ner_feature.feature.names\n",
        "label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VMwALXz6xmj"
      },
      "outputs": [],
      "source": [
        "#decoding the labels we saw earlier and prints line 1 and line 2 of the training set with the labels\n",
        "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "line1 = \"\"\n",
        "line2 = \"\"\n",
        "for word, label in zip(words, labels):\n",
        "    full_label = label_names[label]\n",
        "    max_length = max(len(word), len(full_label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
        "\n",
        "print(line1)\n",
        "print(line2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxTGFHWUN5_q",
        "outputId": "fd0722ee-7b98-44ae-c7fc-fe05e332aeb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model path in the gdrive\n",
        "MODEL_PATH = \"/content/gdrive/MyDrive/IBO_BETA/IgboBert\" #replace with the model path you want to work with"
      ],
      "metadata": {
        "id": "qF6miayvN997"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Processing the data**"
      ],
      "metadata": {
        "id": "Dlcsd6ego863"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpsUuprr6xm1"
      },
      "outputs": [],
      "source": [
        "#we will be using a IgboBERT pretrained model.\n",
        "#Download and cache the associated tokenizer\n",
        "\n",
        "#You can replace the model_checkpoint with any other model you prefer from the Hub, \n",
        "#or with a local folder in which you’ve saved a pretrained model and a tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = MODEL_PATH\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,add_prefix_space=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN6FSZVg6xm4",
        "outputId": "d58ee397-d0d2-4925-9346-59f4ad57b61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'ĠIke',\n",
              " 'Ġá»ĭda',\n",
              " 'Ġjá»¥á»¥',\n",
              " 'Ġotá»¥',\n",
              " 'Ġnkeji',\n",
              " 'Ġbanyere',\n",
              " 'Ġoke',\n",
              " 'Ġogbugbu',\n",
              " 'Ġna',\n",
              " 'Ġ-',\n",
              " 'Ġeme',\n",
              " 'Ġn',\n",
              " \"'\",\n",
              " 'ala',\n",
              " 'ĠNaijiria',\n",
              " 'Ġagwá»¥la',\n",
              " 'ĠEkweremmadá»¥',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Tokenizing our pre-tokenized input with our tokenizer\n",
        "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "inputs.tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3PnMPLC6xm7"
      },
      "outputs": [],
      "source": [
        "# Expanding our label list to match the tokens and assigning a label of -100 to special tokens \n",
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f01_kcW6xm8",
        "outputId": "e050dad0-6de6-43d3-95cb-808a1f83983c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1]\n",
            "[-100, 0, 0, 0, 7, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, -100]\n"
          ]
        }
      ],
      "source": [
        "# trying out the above line of code on our first sentence to see if -100 was assigned to the special tokens\n",
        "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKTZdsXA6xm-"
      },
      "outputs": [],
      "source": [
        "#we tokenize all the inputs and apply align_labels_with_tokens() on all the labels\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoVnXRos6xnI"
      },
      "outputs": [],
      "source": [
        "#applying all that preprocessing in one go on the other splits of our dataset\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuning the model with the Trainer API**"
      ],
      "metadata": {
        "id": "jp_5MTZfoXRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Azab97cg6xnJ"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JqOCR3l6xni",
        "outputId": "c2b2d847-b271-465e-8d6f-1d73f213b8ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-100,    0,    0,    0,    7,    8,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    5,    0,    1, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100],\n",
              "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    5,    0,    1,\n",
              "            2,    0,    0,    0,    0,    0,    0,    0,    0,    7,    8,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
              "            0,    0,    0,    0, -100]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
        "batch[\"labels\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To have the Trainer compute a metric every epoch, we will need to define a compute_metrics() function that takes the arrays of predictions and    labels, and returns a dictionary with the metric names and values.To use this metric, we first need to install the seqeval library."
      ],
      "metadata": {
        "id": "pYaw34B7UQzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aWCbouj6xnm"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iCqWQOg6xnn",
        "outputId": "3c5896a8-74fc-4fb9-aca0-9784f8a1b86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e546942cc5d94e0699a9f02403254807",
            "01bc978ee0fc4718a0d54d7eb6b149d5",
            "c3d9d8f08d954cf884ab7e45bc20a86d",
            "6261afbb76404083871e1018ee43c4d3",
            "d2963075ecc940289ce243ff4962ada2",
            "26348f3fce06446a93416b723616d3fd",
            "b5479937b33f436ab1a64062e075ba92",
            "e33541fed8164a029141c473a51e876f",
            "a5ad84a8d9754a3da8f1a9bbe97de6d7",
            "3b4e15c8c19d427282c56dfe01abe414",
            "b558e14fd3ab46ef9e99e53ae4ea205d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e546942cc5d94e0699a9f02403254807",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"seqeval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwDTTlvP6xoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orPhVcYa6xoQ"
      },
      "outputs": [],
      "source": [
        "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcHZsgw26xoT",
        "outputId": "96e22372-4376-4719-9077-b4cfd5b20801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/gdrive/MyDrive/IBO_BETA/IgboBert were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at /content/gdrive/MyDrive/IBO_BETA/IgboBert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#defining the model we want to finetune\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV2obmRw6xoY"
      },
      "outputs": [],
      "source": [
        "#log in to Hugging Face \n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpmmlcD_6xoc"
      },
      "outputs": [],
      "source": [
        "#we define our TrainingArguments\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"IgboBert-finetuned-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    #push_to_hub=True, (uncomment if you want to upload your results to the Model Hub)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU5aiBv16xod"
      },
      "outputs": [],
      "source": [
        "#we pass everything to the Trainer and start training\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the fine-tuned model**"
      ],
      "metadata": {
        "id": "PyZLKEebQq5k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enusDmeV6xq0",
        "outputId": "e162ed4e-e805-4e29-8f40-f3ae9d35ccc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/IgboBert-finetuned-ner/checkpoint-5600/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/IgboBert-finetuned-ner/checkpoint-5600\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-PER\",\n",
            "    \"2\": \"I-PER\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"I-ORG\",\n",
            "    \"5\": \"B-LOC\",\n",
            "    \"6\": \"I-LOC\",\n",
            "    \"7\": \"B-DATE\",\n",
            "    \"8\": \"I-DATE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-DATE\": \"7\",\n",
            "    \"B-LOC\": \"5\",\n",
            "    \"B-ORG\": \"3\",\n",
            "    \"B-PER\": \"1\",\n",
            "    \"I-DATE\": \"8\",\n",
            "    \"I-LOC\": \"6\",\n",
            "    \"I-ORG\": \"4\",\n",
            "    \"I-PER\": \"2\",\n",
            "    \"O\": \"0\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading configuration file /content/IgboBert-finetuned-ner/checkpoint-5600/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/content/IgboBert-finetuned-ner/checkpoint-5600\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-PER\",\n",
            "    \"2\": \"I-PER\",\n",
            "    \"3\": \"B-ORG\",\n",
            "    \"4\": \"I-ORG\",\n",
            "    \"5\": \"B-LOC\",\n",
            "    \"6\": \"I-LOC\",\n",
            "    \"7\": \"B-DATE\",\n",
            "    \"8\": \"I-DATE\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-DATE\": \"7\",\n",
            "    \"B-LOC\": \"5\",\n",
            "    \"B-ORG\": \"3\",\n",
            "    \"B-PER\": \"1\",\n",
            "    \"I-DATE\": \"8\",\n",
            "    \"I-LOC\": \"6\",\n",
            "    \"I-ORG\": \"4\",\n",
            "    \"I-PER\": \"2\",\n",
            "    \"O\": \"0\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.15.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n",
            "loading weights file /content/IgboBert-finetuned-ner/checkpoint-5600/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
            "\n",
            "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at /content/IgboBert-finetuned-ner/checkpoint-5600.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
            "Didn't find file /content/IgboBert-finetuned-ner/checkpoint-5600/added_tokens.json. We won't load it.\n",
            "loading file /content/IgboBert-finetuned-ner/checkpoint-5600/vocab.json\n",
            "loading file /content/IgboBert-finetuned-ner/checkpoint-5600/merges.txt\n",
            "loading file /content/IgboBert-finetuned-ner/checkpoint-5600/tokenizer.json\n",
            "loading file None\n",
            "loading file /content/IgboBert-finetuned-ner/checkpoint-5600/special_tokens_map.json\n",
            "loading file /content/IgboBert-finetuned-ner/checkpoint-5600/tokenizer_config.json\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'end': 6,\n",
              "  'entity_group': 'ORG',\n",
              "  'score': 0.99998724,\n",
              "  'start': 0,\n",
              "  'word': ' Google'},\n",
              " {'end': 21,\n",
              "  'entity_group': 'DATE',\n",
              "  'score': 0.99948347,\n",
              "  'start': 15,\n",
              "  'word': ' ụbọchị'},\n",
              " {'end': 33,\n",
              "  'entity_group': 'PER',\n",
              "  'score': 0.99567986,\n",
              "  'start': 28,\n",
              "  'word': ' Keshi'},\n",
              " {'end': 47,\n",
              "  'entity_group': 'PER',\n",
              "  'score': 0.99999,\n",
              "  'start': 39,\n",
              "  'word': ' Obasanjo'},\n",
              " {'end': 62,\n",
              "  'entity_group': 'LOC',\n",
              "  'score': 0.9999815,\n",
              "  'start': 54,\n",
              "  'word': ' Naịjirịa'},\n",
              " {'end': 72,\n",
              "  'entity_group': 'DATE',\n",
              "  'score': 0.9996765,\n",
              "  'start': 63,\n",
              "  'word': ' afọ asatọ'},\n",
              " {'end': 146,\n",
              "  'entity_group': 'LOC',\n",
              "  'score': 0.99975836,\n",
              "  'start': 132,\n",
              "  'word': ' Legọọsi steeti'},\n",
              " {'end': 162,\n",
              "  'entity_group': 'PER',\n",
              "  'score': 0.99998444,\n",
              "  'start': 150,\n",
              "  'word': ' Rasak Fadipe'},\n",
              " {'end': 241,\n",
              "  'entity_group': 'DATE',\n",
              "  'score': 0.98287994,\n",
              "  'start': 214,\n",
              "  'word': \"'ehihie ụbọchị 24 Jenuwarị,\"},\n",
              " {'end': 246,\n",
              "  'entity_group': 'DATE',\n",
              "  'score': 0.99987113,\n",
              "  'start': 242,\n",
              "  'word': ' 2018'}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"/content/IgboBert-finetuned-ner/checkpoint-5600\"\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "token_classifier(\"Google chetara ụbọchị ọmụmụ Keshi mgbe Obasanjo chịrị Naịjirịa afọ asatọ na ọchịchị onye kwuo uche ya.\"\n",
        "                 \"Onyeisi ndị na-emenyu ọkụ na, Legọọsi steeti bụ Rasak Fadipe ekwuola na onweghi onye nwụrụ n'ime ọkụ ahụ gbara n'ehihie ụbọchị 24 Jenuwarị, 2018.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save the fine-tuned model in gdrive\n",
        "import shutil\n",
        "shutil.move('/content/IgboBert-finetuned-ner/checkpoint-5600','/content/gdrive/MyDrive/IBO_BETA/ LREC FINAL TRAINING')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v6EllTL8Tj8n",
        "outputId": "1c26a272-7972-4ac4-8162-ea09b766164a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/IBO_BETA/ LREC FINAL TRAINING/checkpoint-5600'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Fine-tuned IgboBert  Token Classification  model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e546942cc5d94e0699a9f02403254807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_01bc978ee0fc4718a0d54d7eb6b149d5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3d9d8f08d954cf884ab7e45bc20a86d",
              "IPY_MODEL_6261afbb76404083871e1018ee43c4d3",
              "IPY_MODEL_d2963075ecc940289ce243ff4962ada2"
            ]
          }
        },
        "01bc978ee0fc4718a0d54d7eb6b149d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3d9d8f08d954cf884ab7e45bc20a86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26348f3fce06446a93416b723616d3fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: ",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5479937b33f436ab1a64062e075ba92"
          }
        },
        "6261afbb76404083871e1018ee43c4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e33541fed8164a029141c473a51e876f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2482,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2482,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5ad84a8d9754a3da8f1a9bbe97de6d7"
          }
        },
        "d2963075ecc940289ce243ff4962ada2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3b4e15c8c19d427282c56dfe01abe414",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6.34k/? [00:00&lt;00:00, 177kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b558e14fd3ab46ef9e99e53ae4ea205d"
          }
        },
        "26348f3fce06446a93416b723616d3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5479937b33f436ab1a64062e075ba92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e33541fed8164a029141c473a51e876f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5ad84a8d9754a3da8f1a9bbe97de6d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b4e15c8c19d427282c56dfe01abe414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b558e14fd3ab46ef9e99e53ae4ea205d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}